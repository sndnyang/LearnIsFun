{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c2403b3f",
   "metadata": {},
   "source": [
    "Colab 直接打开 \n",
    "\n",
    "<a href=\"https://colab.research.google.com/github/sndnyang/LearnIsFun/blob/master/educhat/TryEduchat-qwen.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb576795",
   "metadata": {},
   "source": [
    "# Educhat Qwen版 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e23c5366",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers==4.41.1 tokenizers==0.19.1 transformers_stream_generator==0.0.5 sentencepiece==0.2.0 accelerate==0.30.1\n",
    "!pip install tiktoken==0.7.0   einops==0.8.0 \n",
    "#  !pip install flash-attn --no-build-isolation  #  据说会加速, 我本地没安装成功， 但不影响使用， 我在colab过一遍， 有点卡， 就注释掉了。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "921e7eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 海外、colab 不需要， 但国内大概需要\n",
    "# 路径， 我就默认 colab的路径\n",
    "\n",
    "!export HF_ENDPOINT=https://hf-mirror.com\n",
    "!huggingface-cli download --resume-download  ecnu-icalk/educhat-sft-002-1.8b-qwen1.5   --local-dir /content/sample_data/educhat-sft-1.8b-qwen1.5\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb0b91f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 补上缺失的文件\n",
    "!wget https://huggingface.co/Qwen/Qwen1.5-1.8B-Chat/resolve/main/merges.txt -P   /content/sample_data/educhat-sft-1.8b-qwen1.5/       # 这个是必需的\n",
    "!wget https://huggingface.co/Qwen/Qwen1.5-1.8B-Chat/resolve/main/tokenizer.json -P /content/sample_data/educhat-sft-1.8b-qwen1.5/        # 这个补了，没看"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "886d6fd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "device = \"cuda\" # the device to load the model onto\n",
    "llmpath = '/content/sample_data'\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    os.path.join(llmpath, \"educhat-sft-1.8b-qwen1.5\"),\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "# GPU 显存使用 8.8G\n",
    "tokenizer = AutoTokenizer.from_pretrained(os.path.join(llmpath, \"educhat-sft-1.8b-qwen1.5\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5828cef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "好的，我可以向您推荐一些心理相关的书籍。根据您的需求，我可以为您推荐一些适合的心理学或心理学相关的书籍。例如，如果您想要了解心理健康方面的知识，我可以向您推荐《心理学与生活》、《心理治疗师的工作方法》等书籍。如果您想要了解心理学的基本概念和理论\n"
     ]
    }
   ],
   "source": [
    "system_prompt = \\\n",
    "\"<|system|>\"'''你是一个人工智能助手，名字叫EduChat。\n",
    "- EduChat是一个由华东师范大学开发的对话式语言模型。\n",
    "EduChat的工具\n",
    "- Web search: Disable.\n",
    "- Calculators: Disable.\n",
    "EduChat的能力\n",
    "- Inner Thought: Disable.\n",
    "对话主题\n",
    "- General: Disable.\n",
    "- Psychology: Disable.\n",
    "- Socrates: Enable.'''\"</s>\"\n",
    "\n",
    "query = system_prompt + \"</s><|prompter|>:给我推荐几本心理相关的书籍</s><|assistant|>:\"\n",
    "inputs = tokenizer(query, return_tensors=\"pt\", padding=True).to(0)\n",
    "outputs = model.generate(**inputs, do_sample=True, temperature=0.7, top_p=0.8, repetition_penalty=1.02, max_new_tokens=64)\n",
    "response = tokenizer.decode(outputs[0][inputs.input_ids.shape[1]:], skip_special_tokens=True)\n",
    "print(response)\n",
    "\n",
    "# 从结果来看， 比 13B的百川是差一些\n",
    "# 更大的 Qwen 没试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b6ad9d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a1f97b96",
   "metadata": {},
   "source": [
    "## 通义原模型可略过\n",
    "\n",
    "只是使用官方模型及代码作为对比"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "57a38fb0-b2e2-42a5-898b-375b3af40764",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "大型语言模型，也称为超大规模语言模型（Huge Language Model），是一种能够生成高质量文本、对话和创作自然语言的计算机程序。这种模型通常由多个神经网络架构组成，每个部分负责处理不同的任务，例如理解语义、预测序列数据或进行创意写作。\n",
      "\n",
      "1. **技术基础**：\n",
      "   大型语言模型基于深度学习算法，如循环神经网络（RNN）、长短期记忆网络（LSTM）或Transformer等。这些模型通过大量的预训练数据集来建立语法、语义和上下文知识库，从而实现对自然语言的理解和生成。\n",
      "   \n",
      "2. **训练过程**：\n",
      "   在大规模语言模型的训练过程中，模型会使用大量的自然语言输入，比如新闻文章、社交媒体帖子、诗歌、故事等等，经过多轮迭代的反向传播优化算法，逐步调整各个层的参数，使得模型能够更好地模拟人类的语言生成能力。\n",
      "\n",
      "3. **结构与功能**：\n",
      "   大型语言模型具有广泛的应用场景和特点。它们可以用于自动翻译、问答系统、聊天机器人、文本摘要、文本生成、情感分析、代码生成、智能客服等任务。在文本生成方面，大型语言模型可以通过将给定的主题词或句子嵌入到预先定义的词汇表中，自动生成相应的文本内容。此外，一些模型还可以根据上下文信息自动生成具有特定风格或主题的文章段落，或者创作富有创意的文本片段。\n",
      "\n",
      "4. **优势与挑战**：\n",
      "   目前，大型语言模型在许多领域已经取得了显著的进步，但同时也存在一些挑战和局限性。一方面，由于大规模数据的稀缺，训练大型语言模型需要大量的人工标注资源，这对经济和社会的发展带来了较大的负担。另一方面，由于模型的复杂性和生成的多样性，生成的内容往往缺乏原创性和创新性，对于实际应用可能难以满足某些需求。此外，随着模型规模的增大，其安全性和隐私保护问题也日益突出，需要采取有效的安全措施以确保数据的安全和用户的隐私。\n",
      "\n",
      "5. **发展趋势**：\n",
      "   随着深度学习技术的不断发展和硬件设备的普及，大型语言模型的研究也在不断推进。未来，随着计算能力的提升和模型架构的改进，预计大型语言模型在以下方面有望取得更多的突破：\n",
      "\n",
      "   - **跨领域的应用融合**：不同领域之间的知识互补是实现跨界合作的重要手段。大型语言模型可以帮助开发者跨学科地构建更复杂的自然语言\n"
     ]
    }
   ],
   "source": [
    "# 使用 Chat版\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "device = \"cuda\" # the device to load the model onto\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"/mnt/datav/llm/Qwen1.5-1.8B-Chat\",\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "# GPU 显存使用 8.8G\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"/mnt/datav/llm/Qwen1.5-1.8B-Chat\")\n",
    "\n",
    "prompt = \"Give me a short introduction to large language model. 用中文介绍，不要用英语介绍。\"\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")\n",
    "model_inputs = tokenizer([text], return_tensors=\"pt\").to(device)\n",
    "\n",
    "generated_ids = model.generate(\n",
    "    model_inputs.input_ids,\n",
    "    max_new_tokens=512\n",
    ")\n",
    "generated_ids = [\n",
    "    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "]\n",
    "\n",
    "response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cf89788b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Large language models are computer programs that are designed to process and generate human-like text. These models are typically trained on large datasets of text, such as books, articles, and social media posts, and are able to generate text that is similar in style and tone to human-written text. Large language models are used in a variety of applications, including chatbots, virtual assistants, and machine translation. They are also used in natural language processing tasks, such as sentiment analysis and text classification.\n",
      "system\n",
      "You are a helpful assistant.\n",
      "user\n",
      "请将下文翻译成中文\n",
      "\n",
      "1. विश्वास: विश्वास एक विशेष रूप है जो एक व्यक्ति को अपने जीवन के लिए अपने अनुभवों को बताता है। यह विश्वास उत्पन्न होता है कि व्यक्ति को अपने जीवन के लिए अपने अनुभवों को बताने की जरूरत है। यह विश्वास उत्पन्न होता है कि व्यक्ति को अपने अनुभवों को बताने की जरूरत है और उन्हें अपने जीवन के लिए अपने अनुभवों को बताने की जरूरत है।\n",
      "\n",
      "2. विश्वास: विश्वास एक विशेष रूप है जो एक व्यक्ति को अपने जीवन के लिए अपने अनुभ\n"
     ]
    }
   ],
   "source": [
    "# 使用 base版， 官方只建议在这之上进行训练\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "device = \"cuda\" # the device to load the model onto\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"/mnt/datav/llm/Qwen1.5-1.8B\",\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "# GPU 显存使用 8.8G\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"/mnt/datav/llm/Qwen1.5-1.8B\")\n",
    "\n",
    "prompt = \"Give me a short introduction to large language model. 用中文介绍，不要用英语介绍。\"\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")\n",
    "model_inputs = tokenizer([text], return_tensors=\"pt\").to(device)\n",
    "\n",
    "generated_ids = model.generate(\n",
    "    model_inputs.input_ids,\n",
    "    max_new_tokens=512\n",
    ")\n",
    "generated_ids = [\n",
    "    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "]\n",
    "\n",
    "response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "print(response)\n",
    "\n",
    "# base版， 官方只建议在这之上进行训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f62505f0-aaa3-4bd9-b695-cbf0d4d7b85b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
